{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a688ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from websocket import create_connection\n",
    "import threading\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import websocket\n",
    "import datetime\n",
    "from functools import partial\n",
    "import timeit\n",
    "import requests\n",
    "import time\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from sqlalchemy import create_engine\n",
    "import asyncio\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "from Dataset import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475f23b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTC_Data(): \n",
    "    def __init__(self, has_data:bool, print_on_init: bool):\n",
    "        self.has_data = has_data\n",
    "        self.engine   = None\n",
    "        self.df_1min  = None\n",
    "        self.df_5min  = None\n",
    "        self.df_30min = None\n",
    "        self.df_1hr   = None\n",
    "        self.df_4hr   = None\n",
    "        self.df_12hr  = None\n",
    "        self.df_24hr  = None\n",
    "        self.data_que        =[]\n",
    "        self.data_que_5_min  =[]\n",
    "        self.data_que_30_min =[]\n",
    "        self.new_5_min_flag  = False\n",
    "        self.new_30_min_flag = False\n",
    "        \n",
    "        self.buildEngine()\n",
    "        self.readData()\n",
    "        self.get_data_since_last_run(print_on_init,'self', False)\n",
    "        \n",
    "        \n",
    "    def buildEngine(self):\n",
    "        env_vars = {} \n",
    "        with open('src/env.txt') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('#') or not line.strip():\n",
    "                    continue\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                env_vars[key]= value\n",
    "        \n",
    "        self.engine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\"\n",
    "                       .format(user=env_vars['USER'],\n",
    "                               pw=env_vars['PASSWORD'],\n",
    "                               host=env_vars['HOST'],\n",
    "                               db=env_vars['DB']))\n",
    "        print('SQL Engine Built...')\n",
    "        \n",
    "    def readData(self):\n",
    "        if (self.has_data): \n",
    "            self.df_1min  = pd.read_csv('BTC_Ticker_Data_1_Min.csv') \n",
    "            print('1 Min Pulled in...')\n",
    "            self.df_5min  = pd.read_csv('BTC_Ticker_Data_5_Min.csv') \n",
    "            print('5 Min Pulled in...')\n",
    "            self.df_30min = pd.read_csv('BTC_Ticker_Data_30_Min.csv') \n",
    "            print('30 Min Pulled in...')\n",
    "            self.df_1hr   = pd.read_csv('BTC_Ticker_Data_1_Hour.csv') \n",
    "            print('1 hour Pulled in...')\n",
    "            self.df_4hr   = pd.read_csv('BTC_Ticker_Data_4_Hour.csv') \n",
    "            print('4 hour Pulled in...')\n",
    "            self.df_12hr  = pd.read_csv('BTC_Ticker_Data_12_Hour.csv') \n",
    "            print('12 hour Pulled in...')\n",
    "            self.df_24hr  = pd.read_csv('BTC_Ticker_Data_24_Hour.csv')\n",
    "            print('24 hour Pulled in...')\n",
    "            \n",
    "        else:\n",
    "            # Read the data from sql and send to CSV \n",
    "            self.df_1min = pd.read_sql_table('BTC_Ticker_Data', con=self.engine)\n",
    "            print('1 Min Pulled in...')\n",
    "            self.df_5min = pd.read_sql_table('BTC_Ticker_Data_5_Min', con=self.engine)\n",
    "            print('5 Min Pulled in...')\n",
    "            self.df_30min = pd.read_sql_table('BTC_Ticker_Data_30_Min', con=self.engine)\n",
    "            print('30 Min Pulled in...')\n",
    "            self.df_1hr = pd.read_sql_table('BTC_Ticker_Data_1_Hour', con=self.engine)\n",
    "            print('1 hour Pulled in...')\n",
    "            self.df_4hr = pd.read_sql_table('BTC_Ticker_Data_4_Hour', con=self.engine)\n",
    "            print('4 hour Pulled in...')\n",
    "            self.df_12hr = pd.read_sql_table('BTC_Ticker_Data_12_Hour', con=self.engine)\n",
    "            print('12 hour Pulled in...')\n",
    "            self.df_24hr = pd.read_sql_table('BTC_Ticker_Data_24_Hour', con=self.engine)\n",
    "            print('24 hour Pulled in...')\n",
    "            ####################\n",
    "            # create a loop to deal with extra columns for now this works fine Likely will delete this\n",
    "            self.df_1min  = df_1min.iloc[: , 2:]\n",
    "            self.df_5min  = df_5min.iloc[: , 2:]\n",
    "            self.df_30min = df_30min.iloc[: , 2:]\n",
    "            self.df_1hr   = df_1hr.iloc[: , 2:]\n",
    "            self.df_4hr   = df_4hr.iloc[: , 2:]\n",
    "            self.df_12hr  = df_12hr.iloc[: , 2:]\n",
    "            self.df_24hr  = df_24hr.iloc[: , 2:]\n",
    "            ####################\n",
    "            self.print_all_to_CSVs()\n",
    "            \n",
    "    def print_all_to_CSVs(self): \n",
    "        self.df_1min.to_csv('BTC_Ticker_Data_1_Min.csv',index=False)\n",
    "        print('1 Min printed to CSV...')\n",
    "        self.df_5min.to_csv('BTC_Ticker_Data_5_Min.csv',index=False)\n",
    "        print('5 Min printed to CSV...')\n",
    "        self.df_30min.to_csv('BTC_Ticker_Data_30_Min.csv',index=False)\n",
    "        print('30 Min printed to CSV...')\n",
    "        self.df_1hr.to_csv('BTC_Ticker_Data_1_Hour.csv',index=False)\n",
    "        print('1 hour printed to CSV...')\n",
    "        self.df_4hr.to_csv('BTC_Ticker_Data_4_Hour.csv',index=False)\n",
    "        print('4 hour printed to CSV...')\n",
    "        self.df_12hr.to_csv('BTC_Ticker_Data_12_Hour.csv',index=False)\n",
    "        print('12 hour printed to CSV...')\n",
    "        self.df_24hr.to_csv('BTC_Ticker_Data_24_Hour.csv',index=False)\n",
    "        print('24 hour printed to CSV...')\n",
    "\n",
    "    def get_data_since_last_run(self, print_to_CSV: bool, called_by: str, init_data_ques: bool ): \n",
    "        if (called_by == 'web'): ###############################\n",
    "            print('checkpoint 1')\n",
    "        start_time = (self.df_1min.iloc[-1]['Unix Timestamp']+60) * 1000000000 # add 60 to start at first missing point\n",
    "        end_time = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).timestamp() * 1000000000\n",
    "        dfList = []\n",
    "\n",
    "        while start_time < end_time :\n",
    "            print(f\"Retrieving data from {datetime.datetime.utcfromtimestamp(start_time // 1000000000)}...\")\n",
    "\n",
    "            query_string = 'https://api.kraken.com/0/public/Trades?pair=xbtusd&since='+ str(start_time)\n",
    "            resp = requests.get(query_string).json()\n",
    "\n",
    "            # Append new data to dataframe\n",
    "            tempFrame = pd.DataFrame(resp['result']['XXBTZUSD'])\n",
    "            dfList.append(tempFrame)\n",
    "\n",
    "            # Determine last retrieved date\n",
    "            if (start_time == int(resp['result']['last'])):\n",
    "                break\n",
    "            else:            \n",
    "                start_time = int(resp['result']['last'])\n",
    "\n",
    "            # Sleep to avoid being rate limited\n",
    "            time.sleep(1.8)\n",
    "        \n",
    "        if (called_by == 'web'): ###############################\n",
    "            print('checkpoint 2')  \n",
    "        if (len(dfList)!=0):  \n",
    "            # combine to Panadas DF \n",
    "            trade_data = pd.concat(dfList, ignore_index=True)\n",
    "            if (called_by == 'web'): ###############################\n",
    "                print('checkpoint 2.1')\n",
    "            trade_data = trade_data.astype({0: float, 1: float, 2:float})\n",
    "\n",
    "            if (called_by == 'web'): ###############################\n",
    "                print('checkpoint 3')\n",
    "\n",
    "            start_time = (self.df_1min.iloc[-1]['Unix Timestamp']+60)\n",
    "            #TODO needs Some Love\n",
    "            end_time   = int(end_time / 1000000000)\n",
    "            end_time   = end_time-(end_time%60)\n",
    "#             print(end_time)\n",
    "            # hereeee\n",
    "            new_one_min_data =[]\n",
    "            symbol_val   = 'BTCUSD'\n",
    "            open_val, high_val, low_val, close_val, volume_val = 0,0,0,0,0\n",
    "            while start_time < end_time:  \n",
    "                relevant_rows = trade_data[\n",
    "                            (trade_data[2] >= start_time) &\n",
    "                            (trade_data[2] < (start_time + 60) )\n",
    "                            ]\n",
    "                unix_val     = start_time\n",
    "                datetime_val = datetime.datetime.utcfromtimestamp(start_time)\n",
    "                if len(relevant_rows) !=0: \n",
    "                    open_val     = relevant_rows.iloc[0][0]\n",
    "                    close_val    = relevant_rows.iloc[-1][0]\n",
    "                    high_val     = np.max(relevant_rows[0])\n",
    "                    low_val      = np.min(relevant_rows[0])\n",
    "                    volume_val   = round(relevant_rows[1].sum(), 8)   \n",
    "                else :# If there are no trades in the given time frame\n",
    "                    # TODO needs validation here in case its the first item \n",
    "                    open_val     = new_one_min_data[-1][6]\n",
    "                    close_val    = new_one_min_data[-1][6]\n",
    "                    high_val     = new_one_min_data[-1][6]\n",
    "                    low_val      = new_one_min_data[-1][6]\n",
    "                    volume_val   = 0 \n",
    "\n",
    "                # Creates 1 min df \n",
    "                new_row = [unix_val, datetime_val, symbol_val, open_val, high_val, low_val, close_val, volume_val]\n",
    "                new_one_min_data.append(new_row)\n",
    "                # add 1 min to start time \n",
    "                start_time+=60\n",
    "\n",
    "            cols = ['Unix Timestamp','Date','Symbol','Open','High','Low','Close','Volume']\n",
    "            new_one_min_data_DF = pd.DataFrame(new_one_min_data, columns=cols)\n",
    "            # add new Data to old data \n",
    "            new_df_1min = pd.concat([self.df_1min, new_one_min_data_DF], ignore_index=True)\n",
    "\n",
    "            cols = ['Unix Timestamp','Date','Symbol','Open','High','Low','Close','Volume','Time Frame']\n",
    "            # df_5min  #########################\n",
    "            last_5min      = self.df_5min.loc[len(self.df_5min)-1,'Unix Timestamp']\n",
    "            rows_to_add    = self.combine_time_frames(new_df_1min, last_5min, 5, \"5 Min\")\n",
    "            rows_to_add_df = pd.DataFrame(rows_to_add,columns=cols)\n",
    "            new_df_5min    = pd.concat([self.df_5min, rows_to_add_df], ignore_index=True)\n",
    "\n",
    "            # df_30min #########################\n",
    "            last_30min     = self.df_30min.loc[len(self.df_30min)-1,'Unix Timestamp']\n",
    "            rows_to_add    = self.combine_time_frames(new_df_1min, last_30min, 30, \"30 Min\")\n",
    "            rows_to_add_df = pd.DataFrame(rows_to_add,columns=cols)\n",
    "            new_df_30min   = pd.concat([self.df_30min, rows_to_add_df], ignore_index=True)\n",
    "\n",
    "            # df_1hr   #########################\n",
    "            last_1hour     = self.df_1hr.loc[len(self.df_1hr)-1,'Unix Timestamp']\n",
    "            rows_to_add    = self.combine_time_frames(new_df_1min, last_1hour, 60, \"1 Hour\")\n",
    "            rows_to_add_df = pd.DataFrame(rows_to_add,columns=cols)\n",
    "            new_df_1hr     = pd.concat([self.df_1hr, rows_to_add_df], ignore_index=True)\n",
    "\n",
    "            # df_4hr   #########################\n",
    "            last_4hour     = self.df_4hr.loc[len(self.df_4hr)-1,'Unix Timestamp']\n",
    "            rows_to_add    = self.combine_time_frames(new_df_1min, last_4hour, 240, \"4 Hour\")\n",
    "            rows_to_add_df = pd.DataFrame(rows_to_add,columns=cols)\n",
    "            new_df_4hr     = pd.concat([self.df_4hr, rows_to_add_df], ignore_index=True)\n",
    "\n",
    "            # df_12hr  #########################\n",
    "            last_12hour    = self.df_12hr.loc[len(self.df_12hr)-1,'Unix Timestamp']\n",
    "            rows_to_add    = self.combine_time_frames(new_df_1min, last_12hour, 720, \"12 Hour\")\n",
    "            rows_to_add_df = pd.DataFrame(rows_to_add,columns=cols)\n",
    "            new_df_12hr    = pd.concat([self.df_12hr, rows_to_add_df], ignore_index=True)\n",
    "\n",
    "            # df_24hr  #########################\n",
    "            last_24hour    = self.df_24hr.loc[len(self.df_24hr)-1,'Unix Timestamp']\n",
    "            rows_to_add    = self.combine_time_frames(new_df_1min, last_24hour, 1440, \"24 Hour\")\n",
    "            rows_to_add_df = pd.DataFrame(rows_to_add,columns=cols)\n",
    "            new_df_24hr    = pd.concat([self.df_24hr, rows_to_add_df], ignore_index=True)\n",
    "\n",
    "\n",
    "            self.df_1min  = self.apply_indicators_to_whole(new_df_1min)\n",
    "            print('1 Min Updated')\n",
    "            self.df_5min  = self.apply_indicators_to_whole(new_df_5min)\n",
    "            print('5 Min Updated')\n",
    "            self.df_30min = self.apply_indicators_to_whole(new_df_30min)\n",
    "            print('30 Min Updated')\n",
    "            self.df_1hr   = self.apply_indicators_to_whole(new_df_1hr)\n",
    "            print('1 Hour Updated')\n",
    "            self.df_4hr   = self.apply_indicators_to_whole(new_df_4hr)\n",
    "            print('4 Hour Updated')\n",
    "            self.df_12hr  = self.apply_indicators_to_whole(new_df_12hr)\n",
    "            print('12 Hour Updated')\n",
    "            self.df_24hr  = self.apply_indicators_to_whole(new_df_24hr)\n",
    "            print('24 Hour Updated')\n",
    "            if print_to_CSV: \n",
    "                self.print_all_to_CSVs()\n",
    "            if init_data_ques: \n",
    "                self.build_data_ques()\n",
    "        \n",
    "    def combine_time_frames(self, data, first_date, increment, dfString):\n",
    "        new_data =[]\n",
    "        symbol_val   = 'BTCUSD'\n",
    "        open_val, high_val, low_val, close_val, volume_val = 0,0,0,0,0\n",
    "        start_time = first_date + 60\n",
    "        while True: \n",
    "            relevant_rows = data[\n",
    "                        (data['Unix Timestamp'] >= start_time) &\n",
    "                        (data['Unix Timestamp'] < (start_time + (60*increment)) )\n",
    "                        ]\n",
    "            if len(relevant_rows)!=increment:\n",
    "                break\n",
    "            # loop break condition if relevant rows does not have the right amount of rows in it \n",
    "\n",
    "            unix_val     = relevant_rows.iloc[-1]['Unix Timestamp']\n",
    "            datetime_val = relevant_rows.iloc[-1]['Date']\n",
    "            open_val     = relevant_rows.iloc[0]['Open']\n",
    "            high_val     = relevant_rows['High'].max()\n",
    "            low_val      = relevant_rows['Low'].min()\n",
    "            close_val    = relevant_rows.iloc[-1]['Close']\n",
    "            volume_val   = relevant_rows['Volume'].sum()\n",
    "\n",
    "            # Creates new row\n",
    "            new_row = [unix_val, datetime_val, symbol_val, open_val, high_val, low_val, close_val, volume_val, dfString]\n",
    "            new_data.append(new_row)\n",
    "\n",
    "            # add 1 min * increment to start time \n",
    "            start_time+=(60 *increment)\n",
    "\n",
    "        return new_data\n",
    "        \n",
    "    def apply_indicators_to_whole(self, df):\n",
    "        # SMAs #####################\n",
    "        df['MA_5']  = df['Close'].rolling(window=5).mean()\n",
    "        df['MA_8']  = df['Close'].rolling(window=8).mean()\n",
    "        df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
    "        df['MA_13'] = df['Close'].rolling(window=13).mean()\n",
    "        df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "        df['MA_50'] = df['Close'].rolling(window=50).mean()\n",
    "        # RSI #####################\n",
    "        delta = df['Close'].diff()\n",
    "        up = delta.clip(lower=0)\n",
    "        down = -1*delta.clip(upper=0)\n",
    "        ema_up = up.ewm(com=13, adjust=False).mean()\n",
    "        ema_down = down.ewm(com=13, adjust=False).mean()\n",
    "        rs = ema_up/ema_down\n",
    "        df['RSI'] = 100 - (100/(1 + rs))\n",
    "        # MacD #####################\n",
    "        exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "        exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "        macd = exp1 - exp2\n",
    "        df['MACD']= macd\n",
    "        exp3 = macd.ewm(span=9, adjust=False).mean()\n",
    "        df['M_Signal']= exp3\n",
    "        return df\n",
    "    \n",
    "    def build_data_ques(self):\n",
    "        print('Data Que init.....')\n",
    "        num=1\n",
    "        que_5=[]\n",
    "        while True: \n",
    "            point = self.df_1min.iloc[-num]\n",
    "            \n",
    "            if point['Unix Timestamp'] == self.df_5min.iloc[-1]['Unix Timestamp']:\n",
    "                break\n",
    "            que_5.append(point)\n",
    "            num+=1 \n",
    "        if len(que_5) != 0:\n",
    "            que_5.reverse()\n",
    "        # Check if the que is full then create 5 min data frame \n",
    "        self.data_que_5_min = que_5\n",
    "        if len(self.data_que_5_min) == 5: \n",
    "            self.build_5_min()\n",
    "        #########################\n",
    "        num=1\n",
    "        que_30=[]\n",
    "        while True: \n",
    "            point = self.df_1min.iloc[-num]\n",
    "            if point['Unix Timestamp'] == self.df_30min.iloc[-1]['Unix Timestamp']:\n",
    "                break\n",
    "            que_30.append(point)\n",
    "            num+=1 \n",
    "        if len(que_30) != 0:\n",
    "            que_30.reverse()\n",
    "        # Check if the que is full then create 30 min data frame \n",
    "        self.data_que_30_min = que_30\n",
    "        if len(self.data_que_30_min) == 30: \n",
    "            self.build_30_min()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def to_data_que(self, data_point: list):\n",
    "        if len(self.data_que)==0:                      # Empty que just add the point \n",
    "            self.data_que.append(data_point)\n",
    "        else: \n",
    "            if data_point[1] == self.data_que[-1][1]: # In the Same interval: add the point\n",
    "                self.data_que.append(data_point)\n",
    "\n",
    "            else: \n",
    "                # pop the last point                 \n",
    "                # Add to the 1 min data frame \n",
    "                new_point = self.data_que.pop()\n",
    "                self.to_one_min(new_point)\n",
    "                # clear the que \n",
    "                # add the new point \n",
    "                self.data_que.clear()\n",
    "                self.data_que.append(data_point)\n",
    "\n",
    "        \n",
    "    def to_one_min(self, data_point:list):\n",
    "        formated_point = [\n",
    "            int(float(data_point[1])),\n",
    "            datetime.datetime.utcfromtimestamp(int(float(data_point[1]))),\n",
    "            'BTCUSD',\n",
    "            float(data_point[2]),\n",
    "            float(data_point[3]),\n",
    "            float(data_point[4]),\n",
    "            float(data_point[5]),\n",
    "            float(data_point[7])    \n",
    "        ]\n",
    "        print('new 1 min point = ', formated_point)\n",
    "        cols = ['Unix Timestamp','Date','Symbol','Open','High','Low','Close','Volume']\n",
    "        if self.df_1min.iloc[-1]['Unix Timestamp'] == formated_point[0]:\n",
    "            print('same')\n",
    "            self.df_1min = self.df_1min.iloc[:-1 , :]\n",
    "            #TODO needs Some Love  \n",
    "        elif formated_point[0]-60 != self.df_1min.iloc[-1]['Unix Timestamp']:\n",
    "            new_one_min = pd.DataFrame([[\n",
    "                                formated_point[0]-60,\n",
    "                                datetime.datetime.utcfromtimestamp(formated_point[0]-60),\n",
    "                                formated_point[2],\n",
    "                                formated_point[3],\n",
    "                                formated_point[4],\n",
    "                                formated_point[5],\n",
    "                                formated_point[6],\n",
    "                                formated_point[7]\n",
    "                                ]],columns=cols)\n",
    "            self.df_1min = self.df_1min.append(new_one_min, ignore_index=True)\n",
    "            # add point  here too \n",
    "            self.data_que_5_min.append(self.df_1min.iloc[-1])\n",
    "            # check if the 5 min que is full\n",
    "            if len(self.data_que_5_min)== 5: \n",
    "                self.build_5_min()\n",
    "                self.new_5_min_flag = True\n",
    "                # yes then build the data frame \n",
    "\n",
    "            # check if the 30 min que is full \n",
    "            self.data_que_30_min.append(self.df_1min.iloc[-1])\n",
    "            if len(self.data_que_30_min)== 30: \n",
    "                self.build_30_min()\n",
    "                self.new_30_min_flag = True\n",
    "                \n",
    "        new_one_min = pd.DataFrame([formated_point], columns=cols)\n",
    "        self.df_1min = self.df_1min.append(new_one_min, ignore_index=True)\n",
    "        self.df_1min = self.apply_indicators_to_whole(self.df_1min)\n",
    "        \n",
    "        # add point \n",
    "        self.data_que_5_min.append(self.df_1min.iloc[-1])\n",
    "        # check if the 5 min que is full\n",
    "        if len(self.data_que_5_min)== 5: \n",
    "            self.build_5_min()\n",
    "            self.new_5_min_flag = True\n",
    "            # yes then build the data frame \n",
    "            \n",
    "            \n",
    "        # check if the 30 min que is full \n",
    "        self.data_que_30_min.append(self.df_1min.iloc[-1])\n",
    "        if len(self.data_que_30_min)== 30: \n",
    "            self.build_30_min()\n",
    "            self.new_30_min_flag = True\n",
    "        \n",
    "        print('added new point')\n",
    "        \n",
    "        if (self.new_30_min_flag | self.new_5_min_flag):\n",
    "            self.predict_point()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_5_min(self):\n",
    "        print('5min called')\n",
    "        df_5min_que = pd.DataFrame(self.data_que_5_min)\n",
    "\n",
    "        unix_val     = df_5min_que.iloc[-1]['Unix Timestamp']\n",
    "        datetime_val = df_5min_que.iloc[-1]['Date']\n",
    "        symbol_val   = 'BTCUSD'\n",
    "        open_val     = df_5min_que.iloc[0]['Open']\n",
    "        high_val     = df_5min_que['High'].max()\n",
    "        low_val      = df_5min_que['Low'].min()\n",
    "        close_val    = df_5min_que.iloc[-1]['Close']\n",
    "        volume_val   = df_5min_que['Volume'].sum()\n",
    "        dfString     = '5 Min'\n",
    "        cols = ['Unix Timestamp','Date','Symbol','Open','High','Low','Close','Volume','Time Frame']\n",
    "        # Creates new row\n",
    "        new_row = [unix_val, datetime_val, symbol_val, open_val, high_val, low_val, close_val, volume_val, dfString]\n",
    "        df_new_row = pd.DataFrame([new_row], columns=cols)\n",
    "        #appends and sends new row \n",
    "        self.df_5min = self.df_5min.append(df_new_row ,ignore_index=True)\n",
    "        self.df_5min = self.apply_indicators_to_whole(self.df_5min)\n",
    "        # clear the data que \n",
    "        self.data_que_5_min.clear()\n",
    "        \n",
    "    def build_30_min(self):\n",
    "        print('5min called')\n",
    "        df_30min_que = pd.DataFrame(self.data_que_30_min)\n",
    "\n",
    "        unix_val     = df_30min_que.iloc[-1]['Unix Timestamp']\n",
    "        datetime_val = df_30min_que.iloc[-1]['Date']\n",
    "        symbol_val   = 'BTCUSD'\n",
    "        open_val     = df_30min_que.iloc[0]['Open']\n",
    "        high_val     = df_30min_que['High'].max()\n",
    "        low_val      = df_30min_que['Low'].min()\n",
    "        close_val    = df_30min_que.iloc[-1]['Close']\n",
    "        volume_val   = df_30min_que['Volume'].sum()\n",
    "        dfString     = '30 Min'\n",
    "        cols = ['Unix Timestamp','Date','Symbol','Open','High','Low','Close','Volume','Time Frame']\n",
    "        # Creates new row\n",
    "        new_row = [unix_val, datetime_val, symbol_val, open_val, high_val, low_val, close_val, volume_val, dfString]\n",
    "        df_new_row = pd.DataFrame([new_row], columns=cols)\n",
    "        #appends and sends new row \n",
    "        self.df_30min = self.df_30min.append(df_new_row ,ignore_index=True)\n",
    "        self.df_30min = self.apply_indicators_to_whole(self.df_30min)\n",
    "        self.data_que_30_min.clear()\n",
    "        \n",
    "    def predict_point(self,):\n",
    "        print('Predicting on new point .....')\n",
    "        # ADAM place your code here \n",
    "        # Access the last data points by the example below \n",
    "        # if you want more than 1 point then call the below a second time with a -2, -3 and so on. \n",
    "        # you have the most up to date data at this point in the code so the last point will be the newest point \n",
    "        # if the one of the flags is triggered you know theres a new point for that flag \n",
    "        predicted = None\n",
    "        \n",
    "        if self.new_5_min_flag:\n",
    "            test_dataset = LiveData(self.df_5min.iloc[-1], self.df_5min.iloc[-2], self.df_5min.iloc[-3], \n",
    "                                    self.df_30min.iloc[-1], self.df_30min.iloc[-1])\n",
    "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "            for X, Y in test_loader:\n",
    "\n",
    "                X,Y = X.to(device), Y.to(device)\n",
    "                X = X.type(torch.float)\n",
    "                outputs = model(X)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                break\n",
    "\n",
    "        \n",
    "        \n",
    "        # Do not change below these are the flags used for predicting \n",
    "        if (self.new_5_min_flag):\n",
    "            self.new_5_min_flag = False\n",
    "        if (self.new_30_min_flag):\n",
    "            self.new_30_min_flag = False\n",
    "        print('Prediction Outputed ....')\n",
    "        \n",
    "        # returns None if no new 5min point to predict on\n",
    "        return predicted\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1249d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KrakenWebSocketClient:\n",
    "    \"\"\"\n",
    "    Kraken Client Interface\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_class ):\n",
    "        websocket.enableTrace(False)\n",
    "        self.data_class=data_class\n",
    "        ws = websocket.WebSocketApp(\"wss://ws.kraken.com/\",\n",
    "                                         on_message=self.on_message,\n",
    "                                         on_error=self.on_error,\n",
    "                                         on_close=self.on_close)\n",
    "        self.ws = ws\n",
    "        self.ws.on_open = self.on_open\n",
    "        self.ws.run_forever()\n",
    "\n",
    "    def on_message(self,ws, message):\n",
    "        if \"event\" not in message: \n",
    "            formated_point = json.loads(message)\n",
    "            self.data_class.to_data_que(formated_point[1])\n",
    "            \n",
    "        return message\n",
    "\n",
    "    def on_error(self,ws, error):\n",
    "        print(error)\n",
    "        return error\n",
    "\n",
    "    def on_close(self, a,b,c):\n",
    "        self.ws.close()\n",
    "        print(\"### closed ###\")   \n",
    "        # this is where the final data will be pushed to the dbs or something \n",
    "\n",
    "    def run(self, *args):\n",
    "        global driver\n",
    "        driver = True\n",
    "        while driver:\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "#                 I think we can use the below part to stop the code \n",
    "#                 p = input()\n",
    "#                 self.ws.send(p)\n",
    "            except KeyboardInterrupt:\n",
    "                driver = False\n",
    "        time.sleep(1)\n",
    "        self.ws.close()\n",
    "        print(\"thread terminating...\")\n",
    "\n",
    "    def on_open(self,ws):\n",
    "        \n",
    "        self.data_class.get_data_since_last_run(False,'web', True)\n",
    "        print('Last Point in: ',self.data_class.df_1min.iloc[-1]['Date'])\n",
    "        print('Subscribed.....')\n",
    "        self.ws.send('{\"event\":\"subscribe\", \"subscription\":{\"name\":\"ohlc\"}, \"pair\":[\"XBT/USD\"]}')\n",
    "        t = threading.Thread(target=self.run)\n",
    "        t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "655f987d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Engine Built...\n",
      "1 Min Pulled in...\n",
      "5 Min Pulled in...\n",
      "30 Min Pulled in...\n",
      "1 hour Pulled in...\n",
      "4 hour Pulled in...\n",
      "12 hour Pulled in...\n",
      "24 hour Pulled in...\n",
      "Retrieving data from 2021-10-17 00:06:00...\n",
      "Retrieving data from 2021-12-12 20:37:53...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xr/8d3zkbjj52q8w5r9fmsq9j3w0000gn/T/ipykernel_53728/2975380181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPRINT_ON_INIT\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mBTC_D\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBTC_Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHAS_DATA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPRINT_ON_INIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mkraken_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKrakenWebSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBTC_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xr/8d3zkbjj52q8w5r9fmsq9j3w0000gn/T/ipykernel_53728/4114833655.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, has_data, print_on_init)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuildEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_since_last_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_on_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'self'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xr/8d3zkbjj52q8w5r9fmsq9j3w0000gn/T/ipykernel_53728/4114833655.py\u001b[0m in \u001b[0;36mget_data_since_last_run\u001b[0;34m(self, print_to_CSV, called_by, init_data_ques)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;31m# If there are no trades in the given time frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0;31m# TODO needs validation here in case its the first item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                     \u001b[0mopen_val\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnew_one_min_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mclose_val\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mnew_one_min_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mhigh_val\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnew_one_min_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "HAS_DATA = True # Change this flag to flase to pull from sql\n",
    "# need to Test the false\n",
    "\n",
    "PRINT_ON_INIT= False\n",
    "BTC_D=BTC_Data(HAS_DATA,PRINT_ON_INIT)\n",
    "kraken_client = KrakenWebSocketClient(BTC_D)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc05169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    HAS_DATA = True # Change this flag to flase to pull from sql\n",
    "    # need to Test the false\n",
    "\n",
    "    PRINT_ON_INIT= False\n",
    "    BTC_D=BTC_Data(HAS_DATA,PRINT_ON_INIT)\n",
    "    kraken_client = KrakenWebSocketClient(BTC_D)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86bd4334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Engine Built...\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school",
   "language": "python",
   "name": "school"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
